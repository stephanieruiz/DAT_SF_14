{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab\n",
    "==========================\n",
    "Regression and Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installation instructions:       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# describing statistical models and building design matrices\n",
    "!pip install patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore data, estimate statistical models, and perform statistical tests\n",
    "!pip install statsmodels\n",
    "\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from seaborn import plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this dataset shows a set of six numeric survey responses $X_i$ (survey responses)  \n",
    "and a dependent variable $Y$ (perceived supervisor quality)  \n",
    "we want to predict $Y$ from the $X$'s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = pd.read_table('http://www.ats.ucla.edu/stat/examples/chp/p054.txt')\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print x.columns\n",
    "print x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the column names have trailing whitespace, so I fix that by mapping `str.strip` onto the `columns`' names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.columns = x.columns.map(str.strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(x, size = 1.2, aspect=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm = smf.ols('Y ~ X1 + X2 + X3 + X4 + X5 + X6', data=x)\n",
    "fit = lm.fit()\n",
    "print fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove feature w/ lowest (abs) t score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit2 = smf.ols('Y ~ X1 + X2 + X3 + X4 + X6', data=x).fit()\n",
    "print fit2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###note R-sq decreases slightly, but adj R-sq increases slightly\n",
    "The adjusted R2 will penalize you for adding independent variables that do not fit the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> increasing bias, decreasing variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit4 = smf.ols('Y ~ X1 + X3 + X6', data=x).fit()\n",
    "print fit4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ optimal bias-variance point reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit5 = smf.ols('Y ~ X1 + X3', data=x).fit()\n",
    "print fit5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note this model is weaker (lower $Adj. R^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want to see absence of structure in resid scatterplot (\"gaussian white noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit4.resid.plot(style='o', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[a bad residue plot](http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial and Regularized Regression\n",
    "First, credits: many of the examples used come from the brilliant [Peter Prettenhofer](https://twitter.com/pprett) of [DataRobot](http://www.datarobot.com/).<br><br>\n",
    "Second, housekeeping: You may need to update scikit:<br>\n",
    "`$ conda update scikit-learn`<br><br>\n",
    "`$ pip update scikit-learn`<br><br>\n",
    "Third, let's take a minute to discuss [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) and [`make_pipeline`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "[Polynomial regression](http://en.wikipedia.org/wiki/Polynomial_regression) fits a n-th order polynomial to our data using least squares. \n",
    "[Linear regression](http://en.wikipedia.org/wiki/Linear_regression) is a special case of polynomial regression which fits a polynomial of degree=1.<br>\n",
    "To illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(5,5)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "np.random.seed(9)\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "# generate points used to plot\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "n_samples = 100\n",
    "X = np.random.uniform(0, 1, size=n_samples)[:, np.newaxis]\n",
    "y = f(X) + np.random.normal(scale=0.3, size=n_samples)[:, np.newaxis]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(x_plot, f(x_plot), label='ground truth', color='green')\n",
    "ax.scatter(X_train, y_train, label='data', s=100)\n",
    "ax.set_ylim((-2, 2))\n",
    "ax.set_xlim((0, 1))\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Q: Is there a *LINE* that best approximates our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_approximation(est, ax, label=None):\n",
    "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n",
    "    ax.plot(x_plot, f(x_plot), label='ground truth', color='green')\n",
    "    ax.scatter(X_train, y_train, s=100)\n",
    "    ax.plot(x_plot, est.predict(x_plot[:, np.newaxis]), color='red', label=label)\n",
    "    ax.set_ylim((-2, 2))\n",
    "    ax.set_xlim((0, 1))\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.legend(loc='upper right',frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "degree = 3\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "est.fit(X_train, y_train)\n",
    "plot_approximation(est, ax, label='degree=%d' % degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Q: Qualitatively, how would you characterize this fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Your turn:\n",
    "Plot the fit of a polynomial of degree 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fit of a polynomial of degree 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fit of a polynomial of degree 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Q: What happens as we increase the degree of polynomial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Q: Which polynomial should we choose? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_error = np.empty(10)\n",
    "test_error = np.empty(10)\n",
    "for degree in range(10):\n",
    "    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    est.fit(X_train, y_train)\n",
    "    train_error[degree] = mean_squared_error(y_train, est.predict(X_train))\n",
    "    test_error[degree] = mean_squared_error(y_test, est.predict(X_test))\n",
    "\n",
    "plt.plot(np.arange(10), train_error, color='green', label='train')\n",
    "plt.plot(np.arange(10), test_error, color='red', label='test')\n",
    "plt.ylim((0.0, 1e0))\n",
    "plt.ylabel('log(mean squared error)')\n",
    "plt.xlabel('degree')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the degree of the polynomial (our proxy for model complexity), the lower the training error. The testing error decreases too, but it eventually reaches its minimum at a degree of three and then starts increasing at a degree of seven. \n",
    "\n",
    "This phenomenon is called *overfitting*: the model is already so complex that it fits the idiosyncrasies of our training data, idiosyncrasies which limit the model's ability to generalize (as measured by the testing error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the optimal choice for the degree of the polynomial approximation would be between three and six. So when we get some data, we could fit a bunch of polynomials and then choose the one that minimizes MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Hand picking polynomials is hard work, and data scientists are lazy so....\n",
    "...we would like a method that eliminates the need to manually select the degree of the polynomial: we can add a constraint to our linear regression model that constrains the magnitude of the coefficients in the regression model. This constraint is called the regularization term and the technique is often called shrinkage in the statistical community because it shrinks the coefficients towards zero. In the context of polynomial regression, constraining the magnitude of the regression coefficients effectively is a smoothness assumption: by constraining the L2 norm of the regression coefficients we express our preference for smooth functions rather than wiggly functions.\n",
    "\n",
    "A popular regularized linear regression model is Ridge Regression. This adds the L2 norm of the coefficients to the ordinary least squares objective:\n",
    "\n",
    "  $J(\\boldsymbol\\beta) = \\frac{1}{n}\\sum_{i=0}^n (y_i - \\boldsymbol\\beta^T \\mathbf{x}_i')^2 + \\alpha \\|\\boldsymbol\\beta\\|_2$\n",
    "\n",
    "where $\\boldsymbol\\beta$ is the vector of coefficients including the intercept term and $\\mathbf{x}_i'$ is the i-th feature fector including a dummy feature for the intercept. The L2 norm term is weighted by a regularization parameter ``alpha``: if ``alpha=0`` then you recover the Ordinary Least Squares regression model. The larger the value of ``alpha`` the higher the smoothness constraint.\n",
    "\n",
    "Below you can see the approximation of a [``sklearn.linear_model.Ridge``](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) estimator fitting a polynomial of degree nine for various values of ``alpha`` (left) and the corresponding coefficient loadings (right). The smaller the value of ``alpha`` the higher the magnitude of the coefficients, so the functions we can model can be more and more wiggly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax_rows = plt.subplots(4, 2, figsize=(15, 20))\n",
    "\n",
    "def plot_coefficients(est, ax, label=None, yscale='log'):\n",
    "    coef = est.steps[-1][1].coef_.ravel()\n",
    "    if yscale == 'log':\n",
    "        ax.semilogy(np.abs(coef), marker='o', label=label)\n",
    "        ax.set_ylim((1e-1, 1e8))\n",
    "    else:\n",
    "        ax.plot(np.abs(coef), marker='o', label=label)\n",
    "    ax.set_ylabel('abs(coefficient)')\n",
    "    ax.set_xlabel('coefficients')\n",
    "    ax.set_xlim((1, 9))\n",
    "\n",
    "degree = 9\n",
    "alphas = [0.0, 1e-8, 1e-5, 1e-1]\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    est = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\n",
    "    est.fit(X_train, y_train)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='Ridge(alpha=%r) coefficients' % alpha)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization techniques\n",
    "\n",
    "In the above example we used Ridge Regression, a regularized linear regression technique that puts an [L2 norm](http://mathworld.wolfram.com/L2-Norm.html) penalty on the regression coefficients. Another popular regularization technique is the LASSO, a technique which puts an [L1 norm](http://mathworld.wolfram.com/L1-Norm.html) penalty instead. The difference between the two is that the LASSO leads to sparse solutions, driving most coefficients to zero, whereas Ridge Regression leads to dense solutions, in which most coefficients are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "fig, ax_rows = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "degree = 9\n",
    "alphas = [1e-3, 1e-2]\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    est = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=alpha))\n",
    "    est.fit(X_train, y_train)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='Lasso(alpha=%r) coefficients' % alpha, yscale=None)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Further Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A criminologist studying the relationship between income level and assults in U.S. cities (among other things) collected the following data for 2215 communities. The dataset can be found in the [UCI machine learning site](http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized).\n",
    "\n",
    "We are interested in the per capita assult rate and its relation to median income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crime = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00211/CommViolPredUnnormalizedData.txt\", \n",
    "                    header = None, na_values  = '?',\n",
    "                    names = ['communityname', 'state', 'countyCode', 'communityCode', 'fold', 'population', 'householdsize', \n",
    "                             'racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29', \n",
    "                             'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome', 'pctWWage', 'pctWFarmSelf', \n",
    "                             'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire', 'medFamInc', 'perCapInc', 'whitePerCap', \n",
    "                             'blackPerCap', 'indianPerCap', 'AsianPerCap', 'OtherPerCap', 'HispPerCap', 'NumUnderPov', \n",
    "                             'PctPopUnderPov', 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctEmploy', \n",
    "                             'PctEmplManu', 'PctEmplProfServ', 'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', \n",
    "                             'MalePctNevMarr', 'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par', 'PctKids2Par', \n",
    "                             'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids', 'PctWorkMom', 'NumKidsBornNeverMar', \n",
    "                             'PctKidsBornNeverMar', 'NumImmig', 'PctImmigRecent', 'PctImmigRec5', 'PctImmigRec8', \n",
    "                             'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5', 'PctRecImmig8', 'PctRecImmig10', \n",
    "                             'PctSpeakEnglOnly', 'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup', \n",
    "                             'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous', 'PctPersOwnOccup', \n",
    "                             'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR', 'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', \n",
    "                             'PctVacantBoarded', 'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb', \n",
    "                             'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart', 'OwnOccQrange', 'RentLowQ', 'RentMedian', \n",
    "                             'RentHighQ', 'RentQrange', 'MedRent', 'MedRentPctHousInc', 'MedOwnCostPctInc', \n",
    "                             'MedOwnCostPctIncNoMtg', 'NumInShelters', 'NumStreet', 'PctForeignBorn', 'PctBornSameState', \n",
    "                             'PctSameHouse85', 'PctSameCity85', 'PctSameState85', 'LemasSwornFT', 'LemasSwFTPerPop', \n",
    "                             'LemasSwFTFieldOps', 'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop', \n",
    "                             'PolicReqPerOffic', 'PolicPerPop', 'RacialMatchCommPol', 'PctPolicWhite', 'PctPolicBlack', \n",
    "                             'PctPolicHisp', 'PctPolicAsian', 'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz', \n",
    "                             'PolicAveOTWorked', 'LandArea', 'PopDens', 'PctUsePubTrans', 'PolicCars', 'PolicOperBudg', \n",
    "                             'LemasPctPolicOnPatr', 'LemasGangUnitDeploy', 'LemasPctOfficDrugUn', 'PolicBudgPerPop', \n",
    "                             'murders', 'murdPerPop', 'rapes', 'rapesPerPop', 'robberies', 'robbbPerPop', 'assaults', \n",
    "                             'assaultPerPop', 'burglaries', 'burglPerPop', 'larcenies', 'larcPerPop', 'autoTheft', \n",
    "                             'autoTheftPerPop', 'arsons', 'arsonsPerPop', 'ViolentCrimesPerPop', 'nonViolPerPop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit a simple linear regression model to the data with `np.log(crime.assaults)` as the dependent variable and `np.log(crime.medIncome)` as the independent variable. Plot the estimated regression line.\n",
    "\n",
    "4. Test whether there is a linear relationship between `assaults` and `medIncome` at level $\\alpha=0.05$. State the null hypothesis, the alternative, the conclusion and the $p$-value.\n",
    "\n",
    "5. Give a 95% confidence interval for the slope of the regression line. Interpret your interval.\n",
    "\n",
    "6. Report the $R^2$ and the adjusted $R^2$ of the model, as well as an estimate of the variance of the errors in the model.\n",
    "\n",
    "7. Go to [archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized](http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized) and pick out a few other factors that might help you predict `assults`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
