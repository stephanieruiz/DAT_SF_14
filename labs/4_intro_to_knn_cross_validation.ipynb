{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the $k$-NN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply the $k$-NN Algorithm\n",
    "* Using Cross Validation\n",
    "* Apply Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from seaborn import plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common data set to validate classification algorithm's performance is the [Fisher Iris data set](http://en.wikipedia.org/wiki/Iris_flower_data_set), which is commonly included in most stats or machine learning packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets, feature_selection\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the values of k in KNN\n",
    "# we will examin the performance on different k values and explore what value gives the best result\n",
    "n_neighbors = range(1, 51, 2)\n",
    "print n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "iris_df['Target'] = iris.target\n",
    "print 'iris data head:'\n",
    "print iris_df.head()\n",
    "print 'iris describe():'\n",
    "print iris_df.describe()\n",
    "\n",
    "print \"label set: \" + repr(iris_df['Target'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_df.plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's explore the data to get some intuition about it\n",
    "\n",
    "#we'll plot 2x3 figures (why?)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3)\n",
    "\n",
    "colors = ['r','g','b']\n",
    "for i in range(3): \n",
    "    tmp = iris_df[iris_df.Target == i]\n",
    "    tmp.plot(x=0,y=1, kind='scatter', c=colors[i], ax=axes[0,0])\n",
    "\n",
    "for i in range(3): \n",
    "    tmp = iris_df[iris_df.Target == i]\n",
    "    tmp.plot(x=0,y=2, kind='scatter', c=colors[i], ax=axes[0,1])\n",
    "\n",
    "for i in range(3): \n",
    "    tmp = iris_df[iris_df.Target == i]\n",
    "    tmp.plot(x=0,y=3, kind='scatter', c=colors[i], ax=axes[0,2])\n",
    "    \n",
    "for i in range(3): \n",
    "    tmp = iris_df[iris_df.Target == i]\n",
    "    tmp.plot(x=1,y=2, kind='scatter', c=colors[i], ax=axes[1,0])\n",
    "\n",
    "for i in range(3): \n",
    "    tmp = iris_df[iris_df.Target == i]\n",
    "    tmp.plot(x=1,y=3, kind='scatter', c=colors[i], ax=axes[1,1])\n",
    "\n",
    "for i in range(3): \n",
    "    tmp = iris_df[iris_df.Target == i]\n",
    "    tmp.plot(x=2,y=3, kind='scatter', c=colors[i], ax=axes[1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training (and test) set using scikit-learn's train_test_split function\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=12)\n",
    "\n",
    "# Try this sequence again with the following random seed.\n",
    "# observe how it changes the scores of K quite dramatically\n",
    "# X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through each neighbors value from 1 to 51 and append\n",
    "# the scores\n",
    "scores = []\n",
    "for n in n_neighbors:\n",
    "    clf = neighbors.KNeighborsClassifier(n)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(n_neighbors, scores, linewidth=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Why does the classification rate go down with more neighbors?\n",
    "\n",
    "\n",
    "\n",
    "#If we have N points in our dataset, what would happen if we use N neighbors\n",
    "#to classify each point\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work above shows that at 11 neighbors, we can get an ideal result that doesn't overfit the data. To verify this, we'll use cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "clf = neighbors.KNeighborsClassifier(11, weights='uniform')\n",
    "clf.fit(iris.data, iris.target)\n",
    "scores = cross_val_score(clf, iris_df.values, iris.target, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scores\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizaiton of the Decision Boundary between Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will just consider the last two features of the dataset for this visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier(11, weights='uniform')\n",
    "clf.fit(iris.data[:, 2:4], iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = 0.01  # step size in the mesh\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "x_min, y_min = iris_df.min()[['petal length (cm)', 'petal width (cm)']]\n",
    "x_max, y_max = iris_df.max()[['petal length (cm)', 'petal width (cm)']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [np.meshgrid](http://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html) (build grid)\n",
    "* [ravel](http://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html) (flatten)\n",
    "* [np.c_](http://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.c_.html#numpy.c_)\n",
    "    * `np.c_[np.array([1,2,3]), np.array([4,5,6])]` will get `[[1, 4],[2, 5],[3, 6]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot also the training points\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "plt.scatter(iris_df['petal length (cm)'], iris_df['petal width (cm)'], c=iris.target, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = {}, weights = '{}')\".format(clf.n_neighbors, clf.weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_norm = pd.DataFrame(scale(iris.data), columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rerun the [parameter search](#Parameter-Search) with `random_state=8`. Do you get the same result for the optimal $k$\n",
    "2. Rerun the whole lab but using [scaled](#Scaling) data\n",
    "3. (Advanced) Write your own `classifyByKNeighbors` method:\n",
    "```\n",
    "score = classifyByKNeighbors(k, X_train, y_train, X_test, y_test)\n",
    "```\n",
    "or even better, your own `MyKNeighborsClassifier` class:\n",
    "```\n",
    "clf = MyKNeighborsClassifier(k)\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
